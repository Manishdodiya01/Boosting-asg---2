{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684465c5-9842-4ca0-b380-219e47bdf91b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df9195-5c2c-4a86-9c7c-b88dda465c50",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a powerful machine learning technique used for both regression and classification problems. It's an ensemble learning method, which means it combines the predictions from multiple individual models (in this case, decision trees) to make a more accurate prediction than any single model.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **Base Learners (Decision Trees):** The basic building blocks of Gradient Boosting are decision trees. These are shallow trees that make decisions based on a set of rules. In each iteration of the algorithm, a new decision tree is trained to predict the errors (the differences between the actual values and the predictions made so far).\n",
    "\n",
    "2. **Sequential Learning:** Unlike traditional decision trees which are built independently, in Gradient Boosting, trees are built sequentially. Each new tree is trained to correct the errors made by the previous ones. This process continues for a set number of iterations or until a specified stopping criterion is met.\n",
    "\n",
    "3. **Weighted Sum of Predictions:** The final prediction is obtained by summing up the predictions from all the individual trees, each weighted by a factor that depends on its accuracy. Essentially, trees that make fewer errors have more influence on the final prediction.\n",
    "\n",
    "4. **Regularization:** To prevent overfitting, Gradient Boosting usually involves some form of regularization. This can be achieved by controlling the maximum depth of the trees, the minimum number of samples required to split a node, or using techniques like shrinkage (also known as learning rate).\n",
    "\n",
    "5. **Loss Function:** The algorithm minimizes a loss function, which is a measure of how well the model is performing. For regression problems, this loss function is typically the Mean Squared Error (MSE), which measures the average squared difference between actual and predicted values.\n",
    "\n",
    "The \"Gradient\" in Gradient Boosting refers to the fact that the algorithm uses gradient descent optimization to minimize the loss function. It does this by adjusting the predictions of the model in the direction that minimizes the loss.\n",
    "\n",
    "Overall, Gradient Boosting Regression is a powerful technique known for its high predictive accuracy. It's widely used in various fields including finance, healthcare, and data science competitions. Some popular implementations of Gradient Boosting include XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271783fe-7079-471e-bf29-8fff5d7ad94a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e36a43-5e56-4dc3-b7b5-400d1a6e04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d073e4a9-3c41-4a2d-964b-351962db977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = make_regression(random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47876f92-b75f-4d37-bbc6-78ea335f557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = GradientBoostingRegressor(random_state=42)\n",
    "reg.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e092db-119c-47e8-bd56-0e895f530c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ff9008-7e55-4380-a067-a5a008a55e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5230823349678804\n",
      "accuracy_score None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score , confusion_matrix , r2_score\n",
    "\n",
    "print(\"accuracy_score\" , print(r2_score(y_pred , y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0ab517-f13a-4674-80bc-30d6f6e88ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13152.667321111327"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_pred , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88344887-7ebf-41cd-91ce-bdce42009cf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e34cbed-4a5a-4c77-834d-47ab970e0685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.5],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.5],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.5],\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'learning_rate' : [0.1 , 0.01 , 0.5],\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [2,3,4]\n",
    "}\n",
    "\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "clf = GridSearchCV(gb_reg, param_grid=param_grid,cv=3 , scoring='neg_mean_squared_error')\n",
    "clf.fit(X_train , y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1fd118-d523-4b5c-b883-1098c81c9f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cb400d-fb29-4f26-bd5d-13e36b0d3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43de807-9667-48e1-9677-b72050e7f21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.5, max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.5, max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.5, max_depth=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_new_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef79a3b-0a17-480d-bf9a-2c8c07dcaba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -31.90493116,  -92.22761531,   59.27569627,  141.3140634 ,\n",
       "        213.63331149,  -80.50868218,  -64.32116482,  233.32807598,\n",
       "        -77.82235467,  -58.37914615,   88.64076926, -113.20451972,\n",
       "        -38.16459473,   18.8009615 ,  -40.69028267,  114.06093255,\n",
       "         84.73331128,   19.31079889, -108.63739271,  251.40813935])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_new_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde9900-dd57-4ae2-9484-f44e1f5826b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04e4f2-ccef-43c7-a302-0f3f62beb917",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a base model that is slightly better than random guessing for a particular problem. Specifically, it's a model that performs only slightly better than chance on its own, but when combined with other weak learners in an ensemble (via boosting), it contributes to the creation of a strong predictive model.\n",
    "\n",
    "In Gradient Boosting, decision trees are commonly used as weak learners. These are shallow trees with a limited number of nodes and splits. Each tree, on its own, might not provide highly accurate predictions. However, when combined with many other trees in the ensemble, they can collectively form a powerful predictive model.\n",
    "\n",
    "The concept of using weak learners in ensemble methods like Gradient Boosting is based on the observation that even weak models can contribute valuable information to the overall prediction, especially if they focus on different aspects or nuances of the data. Through the boosting process, the subsequent weak learners are trained to correct the errors of the preceding ones, gradually improving the model's accuracy.\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to effectively combine multiple weak learners to create a strong, accurate predictive model. This contrasts with methods like bagging (used in Random Forests), where the base models can be more complex (e.g., deep decision trees) and are trained independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14b02b-500c-4666-bf3a-c5f1b3daf4e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1051f9-d9fc-4211-867c-74994948be16",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be broken down into several key ideas:\n",
    "\n",
    "1. **Sequential Correction of Errors:** Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially. Each new learner is trained to correct the errors made by the previous ones. This is done by focusing on the residuals, which are the differences between the actual target values and the predictions made by the current ensemble.\n",
    "\n",
    "2. **Gradient Descent Optimization:** The \"Gradient\" in Gradient Boosting refers to the fact that the algorithm uses gradient descent optimization to minimize a loss function. This involves finding the direction in which the loss function decreases most rapidly and adjusting the predictions in that direction. In simple terms, it's like trying to descend a hill by taking steps in the steepest downhill direction.\n",
    "\n",
    "3. **Weighted Combination of Models:** Each weak learner contributes to the final prediction by assigning it a weight based on its performance. Learners that make fewer errors have more influence on the final prediction. This means that the algorithm learns which models are more reliable and should be given more weight in the ensemble.\n",
    "\n",
    "4. **Regularization for Robustness:** Gradient Boosting often involves the use of techniques like shrinkage (learning rate) and controlling the complexity of the base models (e.g., by limiting the depth of the trees). These measures help prevent overfitting and make the model more robust to noise in the data.\n",
    "\n",
    "5. **Capturing Complex Relationships:** By combining many weak models, Gradient Boosting can capture complex relationships in the data that might be difficult for a single model to learn. Each weak learner focuses on a different aspect of the data, allowing the ensemble to collectively understand a wide range of patterns.\n",
    "\n",
    "6. **Versatility and High Accuracy:** Gradient Boosting is a versatile and powerful algorithm that can be applied to a wide range of tasks, including regression and classification. It's known for its high predictive accuracy and is widely used in various domains, including finance, healthcare, and machine learning competitions.\n",
    "\n",
    "Overall, the intuition behind Gradient Boosting lies in its ability to iteratively refine its predictions by learning from the errors of previous models. This makes it a highly effective technique for building accurate predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b9777-3e58-47c3-b392-487c66a6f9e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d2352-ae12-405a-b628-5c83e52512e0",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners (usually decision trees) through a process that involves several key steps:\n",
    "\n",
    "1. **Initialization:** The process begins by initializing the ensemble with a simple model, often a tree with just one node (also called a \"stump\"). This initial model makes predictions based on the average target value of the training data.\n",
    "\n",
    "2. **Compute Residuals:** After making predictions with the initial model, the algorithm calculates the residuals, which are the differences between the actual target values and the predictions. These residuals represent the errors made by the initial model.\n",
    "\n",
    "3. **Train a Weak Learner:** The next step involves training a weak learner (a shallow decision tree) on the residuals. This new tree is trained to predict these residuals. The goal is to find patterns or relationships in the data that can help correct the errors made by the initial model.\n",
    "\n",
    "4. **Update Predictions:** The predictions of the ensemble are updated by adding the predictions of the new weak learner, weighted by a factor that depends on the learning rate (also known as the shrinkage). The learning rate controls the contribution of each weak learner to the overall prediction.\n",
    "\n",
    "5. **Repeat Steps 2-4:** Steps 2 to 4 are repeated for a specified number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is trained on the residuals, and its predictions are added to the ensemble.\n",
    "\n",
    "6. **Final Prediction:** The final prediction is obtained by summing up the predictions from all the weak learners in the ensemble, each weighted by the learning rate. This aggregated prediction represents the output of the Gradient Boosting model.\n",
    "\n",
    "7. **Regularization (Optional):** Optionally, regularization techniques like controlling the maximum depth of the trees or using subsampling (only using a subset of the data for training) may be applied to prevent overfitting.\n",
    "\n",
    "8. **Loss Function Optimization:** Throughout the process, the algorithm is minimizing a loss function (e.g., Mean Squared Error for regression tasks) to ensure that the ensemble's predictions are as accurate as possible.\n",
    "\n",
    "By iteratively training weak learners to correct the errors of the previous models, Gradient Boosting builds a powerful ensemble that can capture complex relationships in the data. Each weak learner focuses on a different aspect of the data, allowing the ensemble to collectively understand a wide range of patterns. This results in a highly accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cf5f7-7452-4777-8769-ab278f0b2114",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f59d51-cabf-4334-9e56-72c74818fd4d",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding how it minimizes a loss function by iteratively fitting weak learners to the residuals. Here are the steps involved:\n",
    "\n",
    "1. **Initialize with a Simple Model:**\n",
    "   - Start with an initial model \\(F_0(x)\\) that makes predictions based on the average target value of the training data. This can be represented as:\n",
    "     \\[F_0(x) = \\arg\\min_c \\sum_{i=1}^{N} L(y_i, c)\\]\n",
    "   where \\(L\\) is the loss function (e.g., Mean Squared Error for regression).\n",
    "\n",
    "2. **Calculate Residuals:**\n",
    "   - Compute the residuals:\n",
    "     \\[r_{i}^{(0)} = y_i - F_0(x_i)\\]\n",
    "\n",
    "3. **Iteratively Fit Weak Learners:**\n",
    "   - For \\(m = 1\\) to \\(M\\) (where \\(M\\) is the total number of iterations):\n",
    "     - Train a weak learner (e.g., a shallow decision tree) \\(h_m(x)\\) to predict the residuals:\n",
    "       \\[h_m(x) = \\arg\\min_h \\sum_{i=1}^{N} L(y_i, F_{m-1}(x_i) + h(x_i))\\]\n",
    "     - Compute the step size (learning rate) \\(\\rho_m\\) that minimizes the loss:\n",
    "       \\[\\rho_m = \\arg\\min_\\rho \\sum_{i=1}^{N} L\\left(y_i, F_{m-1}(x_i) + \\rho h_m(x_i)\\right)\\]\n",
    "     - Update the model:\n",
    "       \\[F_m(x) = F_{m-1}(x) + \\rho_m h_m(x)\\]\n",
    "\n",
    "4. **Final Prediction:**\n",
    "   - The final prediction is the sum of all weak learners weighted by their respective step sizes:\n",
    "     \\[F(x) = \\sum_{m=1}^{M} \\rho_m h_m(x)\\]\n",
    "\n",
    "5. **Regularization (Optional):**\n",
    "   - Optionally, apply regularization techniques like controlling the maximum depth of the trees or using subsampling to prevent overfitting.\n",
    "\n",
    "6. **Loss Function Minimization:**\n",
    "   - Throughout the process, the algorithm is minimizing a loss function \\(L\\) to ensure that the ensemble's predictions are as accurate as possible.\n",
    "\n",
    "The goal of these steps is to find the best combination of weak learners (trees) and their weights that minimizes the overall loss function. Each weak learner is trained to correct the errors made by the previous models, gradually improving the model's accuracy.\n",
    "\n",
    "The process of fitting weak learners to the residuals and updating the ensemble is what gives Gradient Boosting its name. It's a powerful technique for building accurate predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197bf3b-117b-41e0-8d10-b85e31c67e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
